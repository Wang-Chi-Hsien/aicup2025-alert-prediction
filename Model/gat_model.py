# -*- coding: utf-8 -*-
"""
GNN æ¨¡å‹ (GAT) è¨“ç·´æµç¨‹æ¨¡çµ„

æœ¬æ¨¡çµ„å°è£äº† GNN æ¨¡å‹å¾è³‡æ–™æº–å‚™ã€æ¨¡å‹å®šç¾©ã€é è¨“ç·´ã€å¾®èª¿åˆ°
è¡ç”Ÿç‰¹å¾µå°å‡ºçš„å®Œæ•´æµç¨‹ã€‚
"""
import warnings
warnings.filterwarnings('ignore')
from pathlib import Path
import pandas as pd
import numpy as np
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch_geometric.data import Data
from torch_geometric.loader import NeighborLoader
from torch_geometric.nn import GATv2Conv
from torch_scatter import scatter_mean
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import roc_auc_score
import xgboost as xgb

# å¾å°ˆæ¡ˆæ¨¡çµ„ä¸­åŒ¯å…¥
from config import GNNConfig as CONFIG  # GNNè¶…åƒæ•¸ä½¿ç”¨ CONFIG åˆ¥åï¼Œä»¥æœ€å°åŒ–ç¨‹å¼ç¢¼ä¿®æ”¹
from config import ProjectConfig
from Preprocess.feature_engineering import build_gnn_node_features, build_graph_data_with_edge_features


# ==============================================================================
# ğŸ“Œ æ¨¡å‹èˆ‡é¡åˆ¥å®šç¾© (ä¾†è‡ª gat_best.py)
# ==============================================================================

class GAT_Model(torch.nn.Module):
    """GATv2 æ¨¡å‹æ¶æ§‹ï¼Œæ•´åˆäº† GraphMAE çš„ç·¨ç¢¼å™¨-è§£ç¢¼å™¨çµæ§‹èˆ‡ç”¨æ–¼å¾®èª¿çš„åˆ†é¡å™¨ã€‚"""
    def __init__(self, in_channels, hidden_channels, out_channels, edge_dim, heads, num_layers=CONFIG.GNN_LAYERS):
        super().__init__()
        self.encoder = nn.ModuleList()
        # Encoder
        self.encoder.append(GATv2Conv(in_channels, hidden_channels, heads=heads, dropout=CONFIG.DROPOUT_RATE, edge_dim=edge_dim))
        for _ in range(num_layers - 1):
             self.encoder.append(GATv2Conv(hidden_channels * heads, hidden_channels, heads=heads, dropout=CONFIG.DROPOUT_RATE, edge_dim=edge_dim))
        
        encoder_out_dim = hidden_channels * heads
        
        # Decoder (for GraphMAE)
        self.decoder = nn.Sequential(
            nn.Linear(encoder_out_dim, encoder_out_dim),
            nn.GELU(),
            nn.Linear(encoder_out_dim, in_channels)
        )
        self.mask_token = nn.Parameter(torch.zeros(1, in_channels))
        
        # Classifier (for fine-tuning)
        self.classifier = nn.Linear(encoder_out_dim, out_channels)

    def get_embedding(self, x, edge_index, edge_attr):
        """é€šé GAT ç·¨ç¢¼å™¨ç²å–ç¯€é»åµŒå…¥ã€‚"""
        embedding = x
        for conv in self.encoder:
            embedding = F.gelu(conv(embedding, edge_index, edge_attr))
        return embedding

    def forward(self, x, edge_index, edge_attr):
        """ç›£ç£å¼å­¸ç¿’çš„å‰å‘å‚³æ’­ï¼Œè¼¸å‡ºåˆ†é¡ logitsã€‚"""
        embedding = self.get_embedding(x, edge_index, edge_attr)
        embedding_for_classifier = F.dropout(embedding, p=CONFIG.DROPOUT_RATE, training=self.training)
        logits = self.classifier(embedding_for_classifier)
        return logits.squeeze(-1)

    def reconstruct(self, x, edge_index, edge_attr):
        """ç”¨æ–¼è¨ˆç®— gnn_recon_error çš„è¼”åŠ©å‡½æ•¸ã€‚"""
        embedding = self.get_embedding(x, edge_index, edge_attr)
        x_reconstructed = self.decoder(embedding)
        return x_reconstructed

    def pretrain_forward(self, x, edge_index, edge_attr, mask_nodes):
        """GraphMAE é è¨“ç·´çš„å‰å‘å‚³æ’­ï¼Œè¨ˆç®—é‡å»ºæå¤±ã€‚"""
        x_masked = x.clone()
        x_masked[mask_nodes] = self.mask_token
        h = self.get_embedding(x_masked, edge_index, edge_attr)
        h_masked = h[mask_nodes]
        x_recon = self.decoder(h_masked)
        x_original_masked = x[mask_nodes]
        loss = F.cosine_similarity(x_recon, x_original_masked.detach(), dim=1)
        return 1.0 - loss.mean()


class FocalLoss(nn.Module):
    """Focal Loss å¯¦ç¾ï¼Œç”¨æ–¼è™•ç†é¡åˆ¥ä¸å¹³è¡¡å•é¡Œã€‚"""
    def __init__(self, alpha=CONFIG.FOCAL_LOSS_ALPHA, gamma=CONFIG.FOCAL_LOSS_GAMMA, reduction='mean'):
        super(FocalLoss, self).__init__()
        self.alpha = alpha
        self.gamma = gamma
        self.reduction = reduction
    def forward(self, inputs, targets):
        BCE_loss = F.binary_cross_entropy_with_logits(inputs, targets, reduction='none')
        pt = torch.exp(-BCE_loss)
        F_loss = self.alpha * (1 - pt)**self.gamma * BCE_loss
        if self.reduction == 'mean': return torch.mean(F_loss)
        elif self.reduction == 'sum': return torch.sum(F_loss)
        else: return F_loss


# ==============================================================================
# ğŸ“Œ è¨“ç·´èˆ‡è©•ä¼°å‡½å¼ (ä¾†è‡ª gat_best.py)
# ==============================================================================

def pretrain_unsupervised(model, data, device):
    """åŸ·è¡Œ GraphMAE é¢¨æ ¼çš„ç„¡ç›£ç£é è¨“ç·´ã€‚"""
    print("\n--- éšæ®µä¸€ï¼šé–‹å§‹ GraphMAE é¢¨æ ¼çš„ç„¡ç›£ç£é è¨“ç·´ ---")
    model.train()
    optimizer = torch.optim.AdamW(model.parameters(), lr=CONFIG.PRETRAIN_LR, weight_decay=CONFIG.WEIGHT_DECAY)
    
    loader = NeighborLoader(data, input_nodes=None, num_neighbors=[-1] * CONFIG.GNN_LAYERS,
                            batch_size=CONFIG.PRETRAIN_BATCH_SIZE, shuffle=True, num_workers=0)
    
    for epoch in range(1, CONFIG.PRETRAIN_EPOCHS + 1):
        total_loss = 0
        processed_batches = 0
        for batch in loader:
            batch = batch.to(device)
            if batch.num_nodes == 0: continue
            optimizer.zero_grad()
            
            num_center_nodes = batch.batch_size
            perm = torch.randperm(num_center_nodes, device=device)
            num_mask_nodes = int(CONFIG.MASK_RATE * num_center_nodes)
            mask_nodes_local = perm[:num_mask_nodes]

            loss = model.pretrain_forward(batch.x, batch.edge_index, batch.edge_attr, mask_nodes_local)
            
            if loss is not None:
                loss.backward()
                optimizer.step()
                total_loss += loss.item()
                processed_batches += 1
                
        avg_loss = total_loss / processed_batches if processed_batches > 0 else 0
        if epoch % 5 == 0 or epoch == CONFIG.PRETRAIN_EPOCHS:
            print(f"é è¨“ç·´ Epoch {epoch:03d}: å¹³å‡é®è”½é‡å»ºæå¤± (1 - CosineSimilarity): {avg_loss:.6f}")
            
    torch.save(model.state_dict(), ProjectConfig.PRETRAIN_MODEL_PATH)
    print(f"âœ… GraphMAE é è¨“ç·´æ¨¡å‹å·²å„²å­˜è‡³: {ProjectConfig.PRETRAIN_MODEL_PATH}")


def finetune_supervised(model, full_data, masks, device):
    """åŸ·è¡Œç›£ç£å¼å¾®èª¿ï¼ŒåŒ…å«å­¸ç¿’ç‡æ’ç¨‹èˆ‡æ—©åœæ©Ÿåˆ¶ã€‚"""
    print("\n--- éšæ®µäºŒï¼šé–‹å§‹ç›£ç£å¼å¾®èª¿ ---")
    train_mask, val_mask = masks
    
    optimizer = torch.optim.AdamW(model.parameters(), lr=CONFIG.FINETUNE_LR, weight_decay=CONFIG.WEIGHT_DECAY)
    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'max', factor=0.5, patience=5, min_lr=1e-6)
    loss_fn = FocalLoss().to(device)
    
    train_loader = NeighborLoader(full_data, input_nodes=train_mask, num_neighbors=[20, 15], batch_size=CONFIG.FINETUNE_BATCH_SIZE, shuffle=True, num_workers=4)
    eval_loader = NeighborLoader(full_data, input_nodes=val_mask, num_neighbors=[20, 15], batch_size=CONFIG.FINETUNE_BATCH_SIZE * 2, shuffle=False, num_workers=4)
    
    best_val_auc = 0
    patience_counter = 0

    for epoch in range(1, CONFIG.FINETUNE_EPOCHS + 1):
        model.train()
        total_loss = 0
        for batch in train_loader:
            batch = batch.to(device)
            optimizer.zero_grad()
            logits = model(batch.x, batch.edge_index, batch.edge_attr)
            loss = loss_fn(logits[:batch.batch_size], batch.y[:batch.batch_size].float())
            loss.backward()
            optimizer.step()
            total_loss += loss.item()
        
        avg_loss = total_loss / len(train_loader)
        
        # ä¿®æ­£ 2A: æº–å‚™çœŸå¯¦æ¨™ç±¤ä¸¦å‚³éçµ¦ evaluate å‡½å¼
        y_true_val = full_data.y[val_mask]
        val_auc = evaluate(model, eval_loader, device, y_true_val)
        scheduler.step(val_auc)

        current_lr = optimizer.param_groups[0]['lr']
        
        if epoch % 5 == 0 or epoch == CONFIG.FINETUNE_EPOCHS or epoch == 1:
            print(f"å¾®èª¿ Epoch {epoch:03d}: Loss: {avg_loss:.4f}, Val AUC: {val_auc:.4f}, LR: {current_lr:.6f}")
        
        if val_auc > best_val_auc:
            best_val_auc = val_auc
            patience_counter = 0
            torch.save(model.state_dict(), ProjectConfig.FINETUNE_BEST_MODEL_PATH)
            print(f"ğŸš€ æ–°çš„æœ€ä½³é©—è­‰é›† AUC: {best_val_auc:.4f}ï¼æ¨¡å‹å·²å„²å­˜ã€‚")
        else:
            patience_counter += 1
            if patience_counter >= CONFIG.FINETUNE_PATIENCE:
                print(f"--- æ—©åœè§¸ç™¼ï¼åœ¨ {patience_counter} å€‹ epochs å…§ Val AUC æœªæå‡ã€‚---")
                break
    
    print("âœ… å¾®èª¿å®Œæˆã€‚")
    print(f"--- æ­£åœ¨è¼‰å…¥æœ€ä½³æ¨¡å‹ (AUC: {best_val_auc:.4f}) ---")
    model.load_state_dict(torch.load(ProjectConfig.FINETUNE_BEST_MODEL_PATH, map_location=device))


# ä¿®æ­£ 1: ä¿®æ”¹ evaluate å‡½å¼ä»¥é©æ‡‰æ–°ç‰ˆ torch_geometric API
@torch.no_grad()
def evaluate(model, loader, device, y_true):
    """åœ¨çµ¦å®šçš„è³‡æ–™é›†ä¸Šè©•ä¼°æ¨¡å‹ AUC åˆ†æ•¸ã€‚"""
    model.eval()
    all_preds = []
    
    y_true = y_true.cpu() # ç¢ºä¿ y_true åœ¨ CPU ä¸Š

    for batch in loader:
        batch = batch.to(device)
        logits = model(batch.x, batch.edge_index, batch.edge_attr)
        all_preds.append(logits[:batch.batch_size].cpu())
    
    all_preds = torch.cat(all_preds, dim=0).sigmoid()
    
    # æª¢æŸ¥ y_true æ˜¯å¦åŒ…å«å¤šå€‹é¡åˆ¥
    if len(y_true.unique()) < 2:
        print("è­¦å‘Š: è©•ä¼°é›†ä¸­åªå­˜åœ¨ä¸€å€‹é¡åˆ¥ï¼Œç„¡æ³•è¨ˆç®— AUCã€‚è¿”å› 0.0")
        return 0.0
    
    return roc_auc_score(y_true, all_preds)


@torch.no_grad()
def export_xgboost_features(model, full_data, device, acct_idx_to_acct):
    """å¾è¨“ç·´å¥½çš„ GNN ä¸­ç”Ÿæˆ Level-2 ç‰¹å¾µä¾› XGBoost ä½¿ç”¨ã€‚"""
    print("\n--- æ­¥é©Ÿ 4: ç”Ÿæˆç”¨æ–¼ XGBoost Stacking çš„ GNN è¡ç”Ÿ (Level-2) ç‰¹å¾µ ---")
    model.eval()
    data = full_data.to(device)
    num_nodes = data.num_nodes
    
    print("  - æ­£åœ¨è¨ˆç®— gnn_fraud_prob (GNN é¢¨éšªæ©Ÿç‡)...")
    logits = model(data.x, data.edge_index, data.edge_attr)
    gnn_fraud_prob = torch.sigmoid(logits)

    print("  - æ­£åœ¨è¨ˆç®— gnn_recon_error (GraphMAE é‡å»ºèª¤å·®)...")
    pretrain_model = GAT_Model(in_channels=full_data.num_node_features, hidden_channels=CONFIG.HIDDEN_DIM, 
                               out_channels=1, edge_dim=full_data.num_edge_features, heads=CONFIG.GAT_HEADS).to(device)
    pretrain_model.load_state_dict(torch.load(ProjectConfig.PRETRAIN_MODEL_PATH, map_location=device))
    pretrain_model.eval()
    reconstructed_x = pretrain_model.reconstruct(data.x, data.edge_index, data.edge_attr)
    gnn_recon_error = F.mse_loss(data.x, reconstructed_x, reduction='none').mean(dim=1)

    print("  - æ­£åœ¨è¨ˆç®— gnn_alert_neighbor_risk (è­¦ç¤ºé„°å±…é¢¨éšª)...")
    edge_index = data.edge_index
    source_nodes, dest_nodes = edge_index[0], edge_index[1]
    is_alert_node = (data.y == 1)
    is_source_alert = is_alert_node[source_nodes]
    alert_source_nodes = source_nodes[is_source_alert]
    alert_dest_nodes = dest_nodes[is_source_alert]
    risk_from_alert_neighbors = gnn_fraud_prob[alert_source_nodes]
    gnn_alert_neighbor_risk = scatter_mean(risk_from_alert_neighbors, alert_dest_nodes, dim=0, dim_size=num_nodes)
    
    print("  - æ­£åœ¨è¨ˆç®— gnn_flow_risk_imbalance (è³‡é‡‘æµé¢¨éšªä¸å¹³è¡¡åº¦)...")
    in_risk = scatter_mean(gnn_fraud_prob[source_nodes], dest_nodes, dim=0, dim_size=num_nodes)
    out_risk = scatter_mean(gnn_fraud_prob[dest_nodes], source_nodes, dim=0, dim_size=num_nodes)
    gnn_flow_risk_imbalance = out_risk - in_risk

    print("  - æ­£åœ¨æ•´åˆæ‰€æœ‰è¡ç”Ÿç‰¹å¾µ...")
    xgb_features_df = pd.DataFrame({
        'gnn_fraud_prob': gnn_fraud_prob.cpu().numpy(),
        'gnn_recon_error': gnn_recon_error.cpu().numpy(),
        'gnn_alert_neighbor_risk': gnn_alert_neighbor_risk.cpu().numpy(),
        'gnn_flow_risk_imbalance': gnn_flow_risk_imbalance.cpu().numpy(),
    })
    xgb_features_df['acct'] = xgb_features_df.index.map(acct_idx_to_acct)
    xgb_features_df = xgb_features_df.set_index('acct')
    
    out_path = ProjectConfig.GNN_DERIVED_FEATURES_PATH
    xgb_features_df.to_parquet(out_path)
    print(f"\nâœ… GNN è¡ç”Ÿç‰¹å¾µå·²æˆåŠŸå„²å­˜ç‚º Parquet æ ¼å¼è‡³: {out_path}")
    return xgb_features_df


# ==============================================================================
# ğŸ“Œ ä¸»æµç¨‹å‡½å¼
# ==============================================================================

def run_gnn_pipeline():
    """
    åŸ·è¡Œå®Œæ•´çš„ GNN è¨“ç·´èˆ‡ç‰¹å¾µå°å‡ºæµç¨‹ã€‚
    æ­¤å‡½å¼ç­‰åŒæ–¼åŸå§‹ `gat_best.py` çš„ `main` å‡½å¼ï¼Œä½†è¢«å°è£ä»¥ä¾¿æ–¼
    ç”±ä¸»è…³æœ¬ `main.py` èª¿ç”¨ã€‚
    """
    print('--- GNN æµç¨‹é–‹å§‹: è¼‰å…¥èˆ‡æº–å‚™è³‡æ–™ ---')
    txns_raw = pd.read_csv(ProjectConfig.RAW_DIR / 'acct_transaction.csv')
    alerts = pd.read_csv(ProjectConfig.RAW_DIR / 'acct_alert.csv')
    predict_df = pd.read_csv(ProjectConfig.RAW_DIR / 'acct_predict.csv')

    print(f"\n--- æ­£åœ¨éæ¿¾äº¤æ˜“è³‡æ–™ï¼Œåƒ…ä¿ç•™ç‰å±±éŠ€è¡Œå¸³æˆ¶é–“ (type=1) çš„äº¤æ˜“ ---")
    if 'from_acct_type' in txns_raw.columns and 'to_acct_type' in txns_raw.columns:
        original_txns_count = len(txns_raw)
        txns_raw = txns_raw[(txns_raw['from_acct_type'] == 1) & (txns_raw['to_acct_type'] == 1)].copy()
        print(f"âœ… ç¯©é¸å®Œæˆã€‚äº¤æ˜“ç­†æ•¸å¾ {original_txns_count} ç­†ï¼Œå¤§å¹…æ¸›å°‘è‡³ {len(txns_raw)} ç­†ã€‚")
    else:
        print("âš ï¸ è­¦å‘Š: 'acct_transaction.csv' ä¸­æœªæ‰¾åˆ° 'from_acct_type' æˆ– 'to_acct_type' æ¬„ä½ï¼Œå°‡ä½¿ç”¨æ‰€æœ‰äº¤æ˜“è³‡æ–™ã€‚")
    
    txns = txns_raw.rename(columns={'txn_amt':'amount', 'currency_type':'currency', 'channel_type':'channel', 'txn_time':'time_str', 'txn_date':'date_days'})
    if txns['time_str'].dtype == 'int64' or (txns['time_str'].astype(str).str.isnumeric().all()):
        txns['time_str'] = txns['time_str'].astype(str).str.zfill(6).str.replace(r'(\d{2})(\d{2})(\d{2})', r'\1:\2:\3', regex=True)
    min_date_days = txns['date_days'].min()
    start_date = pd.to_datetime('2023-01-01')
    days_offset = txns['date_days'] - min_date_days
    txns['datetime'] = start_date + pd.to_timedelta(days_offset, unit='D') + pd.to_timedelta(txns['time_str'], errors='coerce')
    txns['amount_twd'] = txns['amount'] * txns['currency'].map(ProjectConfig.EXCHANGE_RATES).fillna(1.0)
    txns.sort_values(by='datetime', inplace=True)
    
    print(f"\n--- æ­¥é©Ÿ 1.3: æ ¹æ“šæ™‚é–“ ({ProjectConfig.TIME_SPLIT_RATIO}) åˆ‡åˆ†æ•¸æ“š ---")
    min_ts_day = txns['date_days'].min()
    max_ts_day = txns['date_days'].max()
    split_point_day = min_ts_day + (max_ts_day - min_ts_day) * ProjectConfig.TIME_SPLIT_RATIO
    train_txns = txns[txns['date_days'] <= split_point_day].copy()
    
    all_accts = list(pd.concat([txns['from_acct'], txns['to_acct'], alerts['acct'], predict_df['acct']]).unique())
    acct_to_idx = {acct: i for i, acct in enumerate(all_accts)}
    print(f"ç¯©é¸å¾Œçš„ç¸½ç¨ç«‹å¸³æˆ¶æ•¸é‡ç‚º: {len(all_accts)}")

    max_train_date = train_txns['datetime'].max()
    features_df = build_gnn_node_features(train_txns, all_accts, cutoff_time=max_train_date)
    
    train_alerts = alerts[alerts['event_date'] <= split_point_day].copy().sort_values('event_date')
    
    alert_accts_in_map = train_alerts['acct'][train_alerts['acct'].isin(acct_to_idx)].unique()
    alert_indices = [acct_to_idx[acct] for acct in alert_accts_in_map]
    
    shared_y = torch.zeros(len(all_accts), dtype=torch.long)
    if alert_indices:
        shared_y[torch.tensor(alert_indices)] = 1
    print(f"\nè¨“ç·´æ™‚é–“æ®µå…§å…±æœ‰ {shared_y.sum().item()} å€‹æ­£æ¨£æœ¬ (è­¦ç¤ºå¸³æˆ¶)")
    
    if ProjectConfig.USE_XGB_FEATURE_SELECTION_FOR_GNN:
        print(f"\n--- æ­£åœ¨å•Ÿç”¨ XGBoost é€²è¡Œç‰¹å¾µç¯©é¸ (ç›®æ¨™ Top {CONFIG.INPUT_FEATURES_K}) ---")
        y_np = shared_y.numpy()
        scale_pos_weight = (y_np == 0).sum() / (y_np == 1).sum() if (y_np == 1).sum() > 0 else 1
        baseline_xgb = xgb.XGBClassifier(n_estimators=100, scale_pos_weight=scale_pos_weight, random_state=ProjectConfig.SEED, n_jobs=-1)
        baseline_xgb.fit(features_df, y_np)
        feature_importances = pd.Series(baseline_xgb.feature_importances_, index=features_df.columns)
        top_k_features = feature_importances.sort_values(ascending=False).head(CONFIG.INPUT_FEATURES_K).index.tolist()
        print(f"âœ… å·²ç¯©é¸å‡º Top {len(top_k_features)} å€‹ç‰¹å¾µ:")
        features_df_selected = features_df[top_k_features]
    else:
        print("\n--- å·²è·³é XGBoost ç‰¹å¾µç¯©é¸ï¼Œå°‡ä½¿ç”¨æ‰€æœ‰ç”Ÿæˆçš„ç‰¹å¾µ ---")
        features_df_selected = features_df

    scaler_node = StandardScaler()
    shared_x = torch.tensor(scaler_node.fit_transform(features_df_selected), dtype=torch.float)
    print(f"æœ€çµ‚ GNN è¼¸å…¥çš„ç¯€é»ç‰¹å¾µç¶­åº¦: {shared_x.shape[1]}")

    train_data_graph_parts, _ = build_graph_data_with_edge_features(train_txns, acct_to_idx, scaler_edge=None, all_txns_for_dummies=txns)
    full_data = Data(x=shared_x, y=shared_y, **train_data_graph_parts.to_dict())
    
    print("\n--- æ­£åœ¨ä»¥ã€Œæ™‚é–“åºåˆ—ã€åŠƒåˆ†æœ‰æ¨™ç±¤æ•¸æ“šçš„è¨“ç·´/é©—è­‰é›† ---")
    train_mask = torch.zeros(full_data.num_nodes, dtype=torch.bool)
    val_mask = torch.zeros(full_data.num_nodes, dtype=torch.bool)
    
    if not train_alerts.empty:
        # åªè™•ç†å­˜åœ¨æ–¼ acct_to_idx ä¸­çš„è­¦ç¤ºå¸³æˆ¶
        known_alerts = train_alerts[train_alerts['acct'].isin(acct_to_idx)].copy()
        known_alerts['idx'] = known_alerts['acct'].map(acct_to_idx)
        
        val_ratio = 0.2
        if len(known_alerts) > 1:
            split_index = int(len(known_alerts) * (1 - val_ratio))
            train_set_alerts = known_alerts.iloc[:split_index]
            val_set_alerts = known_alerts.iloc[split_index:]

            train_pos_indices = torch.tensor(train_set_alerts['idx'].values, dtype=torch.long)
            val_pos_indices = torch.tensor(val_set_alerts['idx'].values, dtype=torch.long)
            
            all_pos_indices = torch.cat([train_pos_indices, val_pos_indices])
            all_neg_indices = torch.where(full_data.y == 0)[0]
            
            # ç¢ºä¿è² æ¨£æœ¬ä¸åŒ…å«ä»»ä½•æ­£æ¨£æœ¬
            neg_indices_for_sampling = torch.from_numpy(np.setdiff1d(all_neg_indices.numpy(), all_pos_indices.numpy()))
            
            # ç‚ºè¨“ç·´é›†æŠ½æ¨£è² æ¨£æœ¬
            train_neg_sample_size = min(len(neg_indices_for_sampling), len(train_pos_indices) * 10)
            sampled_train_neg_indices = neg_indices_for_sampling[torch.randperm(len(neg_indices_for_sampling))[:train_neg_sample_size]]
            
            train_mask[train_pos_indices] = True
            train_mask[sampled_train_neg_indices] = True

            # å¾å‰©é¤˜çš„è² æ¨£æœ¬ä¸­ç‚ºé©—è­‰é›†æŠ½æ¨£
            remaining_neg_indices = torch.from_numpy(np.setdiff1d(neg_indices_for_sampling.numpy(), sampled_train_neg_indices.numpy()))
            val_neg_sample_size = min(len(remaining_neg_indices), len(val_pos_indices) * 10)
            sampled_val_neg_indices = remaining_neg_indices[torch.randperm(len(remaining_neg_indices))[:val_neg_sample_size]]
            
            val_mask[val_pos_indices] = True
            val_mask[sampled_val_neg_indices] = True
        else:
             print("è­¦å‘Š: å·²çŸ¥çš„è­¦ç¤ºå¸³æˆ¶æ•¸é‡ä¸è¶³ä»¥åŠƒåˆ†è¨“ç·´/é©—è­‰é›†ã€‚")
    else:
        print("è­¦å‘Š: è¨“ç·´æ™‚é–“æ®µå…§æ²’æœ‰ä»»ä½•æ¨™ç±¤ï¼Œç„¡æ³•å‰µå»ºç›£ç£å¼å¾®èª¿æ‰€éœ€çš„æ•¸æ“šé›†ã€‚")

    print(f"âœ… åŠƒåˆ†å®Œæˆ - è¨“ç·´ç¯€é»: {train_mask.sum()}, é©—è­‰ç¯€é»: {val_mask.sum()}")
    
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    print(f"\nå°‡ä½¿ç”¨è£ç½®: {device}")
    
    full_data = full_data.to(device)
    
    model = GAT_Model(in_channels=full_data.num_node_features, hidden_channels=CONFIG.HIDDEN_DIM, 
                      out_channels=1, edge_dim=full_data.num_edge_features, heads=CONFIG.GAT_HEADS).to(device)
    print(f"\nGNN æ¨¡å‹å·²åˆå§‹åŒ–:\n{model}")
    
    pretrain_unsupervised(model, full_data, device)
    
    if train_mask.sum() > 0 and val_mask.sum() > 0:
        print(f"\n--- è¼‰å…¥é è¨“ç·´æ¨¡å‹ ({ProjectConfig.PRETRAIN_MODEL_PATH}) æº–å‚™å¾®èª¿ ---")
        model.load_state_dict(torch.load(ProjectConfig.PRETRAIN_MODEL_PATH, map_location=device))
        
        masks = (train_mask.to(device), val_mask.to(device))
        finetune_supervised(model, full_data, masks, device)
        
        print("\n--- æœ€çµ‚æ¨¡å‹è©•ä¼° ---")
        final_eval_loader = NeighborLoader(full_data, input_nodes=val_mask.to(device), num_neighbors=[15]*CONFIG.GNN_LAYERS, batch_size=CONFIG.FINETUNE_BATCH_SIZE * 2, shuffle=False, num_workers=0)
        
        # ä¿®æ­£ 2B: æº–å‚™çœŸå¯¦æ¨™ç±¤ä¸¦å‚³éçµ¦ evaluate å‡½å¼
        y_true_final = full_data.y[val_mask.to(device)]
        final_val_auc = evaluate(model, final_eval_loader, device, y_true_final)
        
        print(f"ğŸ‰ æœ€çµ‚é©—è­‰é›†ä¸Šçš„ AUC (ä½¿ç”¨æœ€ä½³æ¨¡å‹): {final_val_auc:.4f}")
    else:
        print("è­¦å‘Š: ç”±æ–¼è¨“ç·´é›†æˆ–é©—è­‰é›†ç‚ºç©ºï¼Œè·³éç›£ç£å¼å¾®èª¿å’Œè©•ä¼°ã€‚")
    
    acct_idx_to_acct = {i: acct for acct, i in acct_to_idx.items()}
    export_xgboost_features(model, full_data, device, acct_idx_to_acct)
    
    print("\nGNN æµç¨‹åŸ·è¡Œå®Œç•¢ã€‚")