# -*- coding: utf-8 -*-
"""
XGBoost æ¨¡å‹è¨“ç·´æµç¨‹æ¨¡çµ„

æœ¬æ¨¡çµ„è² è²¬ XGBoost æ¨¡å‹çš„å®Œæ•´æµç¨‹ï¼ŒåŒ…å«ï¼š
- è¼‰å…¥æ¸…ç†å¾Œçš„è³‡æ–™èˆ‡ GNN è¡ç”Ÿç‰¹å¾µã€‚
- å»ºç«‹è¨“ç·´èˆ‡æ¸¬è©¦è³‡æ–™é›†ã€‚
- åŸ·è¡Œäº¤å‰é©—è­‰è¨“ç·´ã€‚
- éŒ¯èª¤åˆ†æèˆ‡ç‰¹å¾µé‡è¦æ€§ç¹ªåœ–ã€‚
- ç”¢ç”Ÿæœ€çµ‚æäº¤æª”æ¡ˆã€‚
"""
import pandas as pd
import numpy as np
import xgboost as xgb
from sklearn.model_selection import StratifiedKFold
from sklearn.metrics import f1_score, precision_score, recall_score
import matplotlib.pyplot as plt
import warnings
from pathlib import Path
import sys
import pickle

# å˜—è©¦åŒ¯å…¥ SHAPï¼Œå¦‚æœå¤±æ•—å‰‡è¨­ç‚º None
try:
    import shap
except ImportError:
    shap = None

warnings.filterwarnings('ignore')

# å¾å°ˆæ¡ˆæ¨¡çµ„ä¸­åŒ¯å…¥
from config import XGBConfig, ProjectConfig
# ä¿®æ­£ï¼šåŒ¯å…¥æ­£ç¢ºåç¨±çš„ç‰¹å¾µå·¥ç¨‹å‡½å¼
from Preprocess.feature_engineering import read_csv_safely, create_xgb_feature_set


# ==============================================================================
# ğŸ“Œ è¼”åŠ©å‡½å¼ (ä¾†è‡ª xgb_test.py)
# ==============================================================================

def run_simplified_cv(X, y, xgb_params, n_splits=3):
    """åŸ·è¡Œä¸€å€‹ç°¡åŒ–çš„äº¤å‰é©—è­‰ä¾†å¿«é€Ÿè©•ä¼°æ¨¡å‹æ€§èƒ½å’Œç‰¹å¾µé‡è¦æ€§ã€‚"""
    print(f"    - æ­£åœ¨åŸ·è¡Œ {n_splits} æŠ˜äº¤å‰é©—è­‰...")
    skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=88)
    oof_preds = np.zeros(len(X))
    importances = pd.DataFrame(index=X.columns)

    for fold, (train_idx, val_idx) in enumerate(skf.split(X, y)):
        X_train, y_train = X.iloc[train_idx], y.iloc[train_idx]
        X_val, y_val = X.iloc[val_idx], y.iloc[val_idx]

        scale_pos_weight = (y_train == 0).sum() / (y_train == 1).sum() if (y_train == 1).sum() > 0 else 1
        current_params = xgb_params.copy()
        current_params['scale_pos_weight'] = scale_pos_weight

        dtrain = xgb.DMatrix(X_train, label=y_train)
        dval = xgb.DMatrix(X_val, label=y_val)

        model = xgb.train(params=current_params, dtrain=dtrain, num_boost_round=500,
                          evals=[(dval, 'val')], early_stopping_rounds=30, verbose_eval=False)

        oof_preds[val_idx] = model.predict(dval, iteration_range=(0, model.best_iteration))
        importances[f'fold_{fold+1}'] = pd.Series(model.get_score(importance_type='gain')).fillna(0)

    thresholds = np.arange(0.01, 0.51, 0.01)
    f1_scores = [f1_score(y, (oof_preds >= t).astype(int)) for t in thresholds]
    best_f1 = np.max(f1_scores) if f1_scores else 0

    importances['mean'] = importances.mean(axis=1)
    importances.sort_values('mean', ascending=False, inplace=True)

    return best_f1, importances


def perform_shap_analysis(model, X_val, feature_cols, result_dir):
    """è¨ˆç®—ä¸¦å„²å­˜æŒ‡å®šç‰¹å¾µé›†çš„ SHAP å€¼åˆ†æåœ–ã€‚"""
    if shap is None:
        print("  - âš ï¸ SHAP å¥—ä»¶æœªå®‰è£ï¼Œè·³é SHAP åˆ†æã€‚è«‹åŸ·è¡Œ 'pip install shap'ã€‚")
        return
    if not feature_cols:
        print("  - æ‰¾ä¸åˆ°æŒ‡å®šçš„ç‰¹å¾µï¼Œè·³é SHAP åˆ†æã€‚")
        return

    print(f"  - æ­£åœ¨è¨ˆç®— {len(feature_cols)} å€‹ç‰¹å¾µçš„ SHAP å€¼ (ä½¿ç”¨ç¬¬ä¸€æŠ˜çš„æ¨¡å‹)...")
    try:
        explainer = shap.TreeExplainer(model)
        sample_size = min(2000, X_val.shape[0])
        X_val_sample = X_val.sample(sample_size, random_state=42) if sample_size < X_val.shape[0] else X_val
        shap_values = explainer.shap_values(X_val_sample)
        
        feature_indices = [X_val.columns.get_loc(col) for col in feature_cols if col in X_val.columns]
        if not feature_indices:
             print("  - SHAP åˆ†æè­¦å‘Šï¼šæä¾›çš„ç‰¹å¾µåˆ—ä¸åœ¨é©—è­‰é›†ä¸­ã€‚")
             return
             
        shap_values_subset = shap_values[:, feature_indices]
        X_val_sample_subset = X_val_sample.iloc[:, feature_indices]
        
        plt.figure()
        shap.summary_plot(shap_values_subset, X_val_sample_subset, show=False, plot_size=(12, max(8, len(feature_cols)//3)))
        plt.title("SHAP Summary Plot (GNN Features Only)")
        save_path_summary = result_dir / "gnn_shap_summary_plot.png"
        plt.savefig(save_path_summary, bbox_inches='tight')
        plt.close()
        print(f"    - GNN ç‰¹å¾µ SHAP Summary Plot å·²å„²å­˜è‡³: {save_path_summary}")

    except Exception as e:
        print(f"  - âŒ SHAP åˆ†æéç¨‹ä¸­ç™¼ç”ŸéŒ¯èª¤: {e}")


def perform_correlation_analysis(X_tabular, X_gnn, threshold=0.7):
    """è¨ˆç®— GNN embeddings èˆ‡è¡¨æ ¼ç‰¹å¾µçš„ç›¸é—œæ€§ï¼Œä¸¦åªå°å‡ºé«˜åº¦ç›¸é—œçš„ç‰¹å¾µå°ã€‚"""
    if X_gnn.empty or X_tabular.empty:
        print("  - ç¼ºå°‘ GNN æˆ–è¡¨æ ¼ç‰¹å¾µï¼Œè·³éç›¸é—œæ€§åˆ†æã€‚")
        return

    print(f"  - æ­£åœ¨è¨ˆç®— GNN èˆ‡è¡¨æ ¼ç‰¹å¾µçš„ç›¸é—œä¿‚æ•¸ (é–€æª» = {threshold})...")
    try:
        combined_df = pd.concat([X_tabular, X_gnn], axis=1)
        corr_matrix = combined_df.corr().abs()
        cross_corr_matrix = corr_matrix.loc[X_tabular.columns, X_gnn.columns]

        highly_correlated_pairs = cross_corr_matrix[cross_corr_matrix > threshold].stack().reset_index()
        highly_correlated_pairs.columns = ['Tabular_Feature', 'GNN_Feature', 'Correlation']

        if not highly_correlated_pairs.empty:
            print(f"  - ç™¼ç¾ {len(highly_correlated_pairs)} çµ„é«˜åº¦ç›¸é—œçš„ç‰¹å¾µå° (Corr > {threshold}):")
            highly_correlated_pairs.sort_values(by='Correlation', ascending=False, inplace=True)
            with pd.option_context('display.max_rows', None):
                print(highly_correlated_pairs.to_string(index=False))
        else:
            print(f"  - âœ… æœªç™¼ç¾çµ•å°ç›¸é—œä¿‚æ•¸è¶…é {threshold} çš„ç‰¹å¾µå°ï¼Œè¡¨ç¤º GNN æä¾›äº†è¼ƒé«˜çš„ç¨ç«‹è³‡è¨Šã€‚")

    except Exception as e:
        print(f"  - âŒ ç›¸é—œæ€§åˆ†æéç¨‹ä¸­ç™¼ç”ŸéŒ¯èª¤: {e}")


def perform_error_analysis(X, y_true, y_pred_proba, threshold, result_dir):
    """åŸ·è¡ŒéŒ¯èª¤åˆ†æï¼Œæ¯”è¼ƒ FN å’Œ TP æ¨£æœ¬çš„ç‰¹å¾µå·®ç•°ã€‚"""
    print("\n--- æ­¥é©Ÿ 7: æ­£åœ¨åŸ·è¡ŒéŒ¯èª¤åˆ†æ... ---")
    y_pred_binary = (y_pred_proba >= threshold).astype(int)
    fn_mask = (y_true == 1) & (y_pred_binary == 0)
    tp_mask = (y_true == 1) & (y_pred_binary == 1)
    fn_accounts = X[fn_mask]
    tp_accounts = X[tp_mask]
    print(f"åˆ†æå®Œæˆã€‚æ‰¾åˆ° {len(fn_accounts)} å€‹å½é™°æ€§ (FN) å¸³æˆ¶å’Œ {len(tp_accounts)} å€‹çœŸæ­£ä¾‹ (TP) å¸³æˆ¶ã€‚")
    if fn_accounts.empty or tp_accounts.empty:
        print("ç„¡æ³•é€²è¡Œ FN vs. TP åˆ†æï¼Œå› ç‚º FN æˆ– TP æ¨£æœ¬ç‚ºç©ºã€‚")
        return
    fn_means = fn_accounts.mean()
    tp_means = tp_accounts.mean()
    comparison_df = pd.DataFrame({'FN_Mean': fn_means, 'TP_Mean': tp_means})
    comparison_df['Ratio (FN/TP)'] = comparison_df['FN_Mean'] / (comparison_df['TP_Mean'] + 1e-9)
    significant_diffs = comparison_df[(comparison_df['Ratio (FN/TP)'] < 0.9) | (comparison_df['Ratio (FN/TP)'] > 1.1)].copy()
    significant_diffs.sort_values('Ratio (FN/TP)', ascending=True, inplace=True)
    print("\n--- ã€é—œéµæ´å¯Ÿã€‘å½é™°æ€§ (FN) vs. çœŸæ­£ä¾‹ (TP) ç‰¹å¾µæ¨¡å¼å°æ¯” ---")
    pd.set_option('display.float_format', '{:12.2f}'.format)
    if not significant_diffs.empty:
        print(significant_diffs.head(25))
    else:
        print("åœ¨ FN èˆ‡ TP ä¹‹é–“æœªç™¼ç¾é¡¯è‘—ç‰¹å¾µå·®ç•°ã€‚")
    pd.reset_option('display.float_format')


def plot_feature_importance(feature_importances, result_dir):
    """ç”Ÿæˆä¸¦å„²å­˜ç‰¹å¾µé‡è¦æ€§åœ–ã€‚"""
    print("\næ­£åœ¨ç”Ÿæˆç‰¹å¾µé‡è¦æ€§åœ–...")
    result_dir.mkdir(parents=True, exist_ok=True)
    feature_importances['mean'] = feature_importances.mean(axis=1)
    feature_importances.sort_values('mean', ascending=False, inplace=True)
    plt.figure(figsize=(12, 16))
    top_n = min(len(feature_importances), 70)
    plt.barh(feature_importances.index[:top_n], feature_importances['mean'][:top_n])
    plt.gca().invert_yaxis()
    plt.title(f"Top {top_n} Feature Importances (XGBoost)")
    plt.xlabel("Importance (Gain)")
    plt.tight_layout()
    save_path = result_dir / "feature_importance_xgb_advanced.png"
    plt.savefig(save_path)
    plt.close()
    print(f"ç‰¹å¾µé‡è¦æ€§åœ–å·²å„²å­˜è‡³: {save_path}")


# ==============================================================================
# ğŸ“Œ ä¸»æµç¨‹å‡½å¼
# ==============================================================================

def run_xgb_pipeline():
    """
    åŸ·è¡Œå®Œæ•´çš„ XGBoost è¨“ç·´èˆ‡é æ¸¬æµç¨‹ã€‚
    æ­¤å‡½å¼ç­‰åŒæ–¼åŸå§‹ `xgb_test.py` çš„ `train_and_predict` å‡½å¼ï¼Œ
    ä½†è¢«å°è£ä»¥ä¾¿æ–¼ç”±ä¸»è…³æœ¬ `main.py` èª¿ç”¨ã€‚
    """
    # --- æ­¥é©Ÿ 1.5: è¼‰å…¥æ¸…ç†å¾Œçš„è³‡æ–™ ---
    print("--- XGBoost æµç¨‹: è¼‰å…¥æ¸…ç†å¾Œçš„è³‡æ–™... ---")
    alerts_df = read_csv_safely(ProjectConfig.PROCESSED_DIR / "acct_alert.csv")
    predict_df = read_csv_safely(ProjectConfig.PROCESSED_DIR / "acct_predict.csv")
    transactions_df = read_csv_safely(ProjectConfig.PROCESSED_DIR / "acct_transaction.csv")
    rename_map = {'txn_time': 'time', 'currency_type': 'currency', 'channel_type': 'channel', 'txn_amt': 'amount'}
    transactions_df.rename(columns=rename_map, inplace=True)
    all_acct_list = pd.unique(transactions_df[['from_acct', 'to_acct']].values.ravel('K'))
    
    # --- æ­¥é©Ÿ 2: å»ºç«‹è¡¨æ ¼ç‰¹å¾µ ---
    # ä¿®æ­£ï¼šå‘¼å«æ­£ç¢ºçš„å‡½å¼åç¨±
    features = create_xgb_feature_set(transactions_df, all_acct_list, alerts_df)

    # --- æ­¥é©Ÿ 2.5: è¼‰å…¥ä¸¦æ•´åˆ GNN ç‰¹å¾µ ---
    print("\n--- æ­¥é©Ÿ 2.5: è¼‰å…¥ä¸¦æ•´åˆ GNN ç‰¹å¾µ ---")
    gnn_features_loaded = False
    gnn_feature_names = []
    try:
        gnn_features_path = ProjectConfig.GNN_DERIVED_FEATURES_PATH
        if gnn_features_path.exists():
            print(f"  - æ‰¾åˆ° GNN ç‰¹å¾µæª”æ¡ˆï¼Œæ­£åœ¨è®€å–: {gnn_features_path}")
            gnn_features = pd.read_parquet(gnn_features_path)
            
            if gnn_features.index.name != 'acct':
                if 'acct' in gnn_features.columns:
                     gnn_features = gnn_features.set_index('acct')
                else:
                     gnn_features.index.name = 'acct'

            features = features.join(gnn_features, how='left').fillna(0)
            gnn_features_loaded = True
            gnn_feature_names = gnn_features.columns.tolist()
            print(f"  - âœ… æˆåŠŸè¼‰å…¥ä¸¦æ•´åˆ {gnn_features.shape[1]} å€‹ GNN ç‰¹å¾µã€‚")
            print(f"     - è¼‰å…¥çš„ GNN ç‰¹å¾µæ¬„ä½: {gnn_feature_names}")
        else:
            print(f"  - âš ï¸ è­¦å‘Š: åœ¨è·¯å¾‘ '{gnn_features_path}' ä¸­æ‰¾ä¸åˆ° GNN ç‰¹å¾µæª”æ¡ˆï¼Œå°‡ä¸ä½¿ç”¨ GNN ç‰¹å¾µã€‚")
    except Exception as e:
        print(f"  - âŒ éŒ¯èª¤: è¼‰å…¥ GNN ç‰¹å¾µæ™‚ç™¼ç”ŸéŒ¯èª¤: {e}", file=sys.stderr)
        
    # --- æ­¥é©Ÿ 3: æº–å‚™è¨“ç·´èˆ‡æ¸¬è©¦è³‡æ–™é›† ---
    print("\n--- æ­¥é©Ÿ 3: æº–å‚™è¨“ç·´èˆ‡æ¸¬è©¦è³‡æ–™é›† ---")
    labels_df = pd.DataFrame({'acct': alerts_df['acct']}).drop_duplicates()
    labels_df['label'] = 1
    train_data = features.join(labels_df.set_index('acct'), how='left')
    train_data['label'] = train_data['label'].fillna(0).astype(int)
    X = train_data.drop('label', axis=1)
    y = train_data['label']
    X_test = features.reindex(predict_df['acct']).fillna(0)[X.columns]
    print(f"ç¸½è¨“ç·´å¸³æˆ¶æ•¸: {len(X)} (å…¶ä¸­è­¦ç¤ºå¸³æˆ¶: {y.sum()})")
    
    tabular_feature_names = [col for col in features.columns if col not in gnn_feature_names]

    # --- æ­¥é©Ÿ 4: GNN è¨ºæ–·æª¢æŸ¥ (å¯é¸) ---
    # (æ­¤è™•çœç•¥äº†åŸå§‹ç¢¼ä¸­çš„è¨ºæ–·éƒ¨åˆ†ä»¥ç°¡åŒ–ï¼Œæ‚¨å¯ä»¥æ ¹æ“šéœ€è¦åŠ å›ä¾†)
    print("\n--- è·³é GNN ç‰¹å¾µè¨ºæ–·æª¢æŸ¥ä»¥åŠ é€Ÿæµç¨‹ ---")

    # --- æ­¥é©Ÿ 5: äº¤å‰é©—è­‰è¨“ç·´ ---
    print(f"\n--- æ­¥é©Ÿ 5: ä½¿ç”¨ {XGBConfig.N_SPLITS} æŠ˜äº¤å‰é©—è­‰è¨“ç·´æœ€çµ‚ XGBoost æ¨¡å‹... ---")
    skf = StratifiedKFold(n_splits=XGBConfig.N_SPLITS, shuffle=True, random_state=42)
    oof_preds = np.zeros(len(X))
    test_preds = np.zeros(len(X_test))
    feature_importances = pd.DataFrame(index=X.columns)
    
    models = [] # ç”¨æ–¼å„²å­˜ SHAP åˆ†æçš„æ¨¡å‹

    for fold, (train_idx, val_idx) in enumerate(skf.split(X, y)):
        print(f"--- FOLD {fold+1}/{XGBConfig.N_SPLITS} ---")
        X_train, y_train = X.iloc[train_idx], y.iloc[train_idx]
        X_val, y_val = X.iloc[val_idx], y.iloc[val_idx]
        scale_pos_weight = (y_train == 0).sum() / (y_train == 1).sum() if (y_train == 1).sum() > 0 else 1
        
        current_params = XGBConfig.PARAMS.copy()
        current_params['scale_pos_weight'] = scale_pos_weight
        dtrain = xgb.DMatrix(X_train, label=y_train)
        dval = xgb.DMatrix(X_val, label=y_val)
        dtest = xgb.DMatrix(X_test)
        
        model = xgb.train(params=current_params, dtrain=dtrain, num_boost_round=2000,
                          evals=[(dval, 'val')], early_stopping_rounds=100, verbose_eval=500)
        
        if fold == 0:
            models.append(model)
        
        best_iter = model.best_iteration
        oof_preds[val_idx] = model.predict(dval, iteration_range=(0, best_iter))
        test_preds += model.predict(dtest, iteration_range=(0, best_iter)) / XGBConfig.N_SPLITS
        feature_importances[f'fold_{fold+1}'] = pd.Series(model.get_score(importance_type='gain')).fillna(0)
    
    # # åœ¨ CV çµæŸå¾ŒåŸ·è¡Œ SHAP åˆ†æ
    # if gnn_features_loaded and models:
    #     print("\n--- GNN è¨ºæ–· (SHAP Analysis) ---")
    #     # ä½¿ç”¨ç¬¬ä¸€æŠ˜çš„é©—è­‰é›†é€²è¡Œåˆ†æ
    #     _, val_idx_fold0 = next(iter(skf.split(X, y)))
    #     X_val_fold0 = X.iloc[val_idx_fold0]
    #     perform_shap_analysis(models[0], X_val_fold0, gnn_feature_names, ProjectConfig.RESULT_DIR)

    # --- æ­¥é©Ÿ 6: æ±ºå®šæœ€çµ‚é–€æª» ---
    print("\n--- æ­¥é©Ÿ 6: æ±ºå®šæœ€çµ‚é–€æª» ---")
    if XGBConfig.MANUAL_THRESHOLD is not None:
        best_threshold = XGBConfig.MANUAL_THRESHOLD
        print(f"ğŸ“Œ ä½¿ç”¨æ‰‹å‹•è¨­å®šçš„é–€æª»: {best_threshold:.4f}")
    else:
        thresholds = np.arange(0.01, 0.51, 0.01)
        scores = [f1_score(y, (oof_preds >= t).astype(int)) for t in thresholds]
        best_threshold = thresholds[np.argmax(scores)] if scores else 0.5
        print(f"  - åœ¨ {XGBConfig.N_SPLITS} æŠ˜äº¤å‰é©—è­‰ä¸Šæ‰¾åˆ°æœ€ä½³é–€æª»: {best_threshold:.2f}")

    f1_val = f1_score(y, (oof_preds >= best_threshold).astype(int))
    precision_val = precision_score(y, (oof_preds >= best_threshold).astype(int))
    recall_val = recall_score(y, (oof_preds >= best_threshold).astype(int))
    print(f"  - æœ€çµ‚ OOF è©•ä¼°æŒ‡æ¨™: F1={f1_val:.4f}, Precision={precision_val:.4f}, Recall={recall_val:.4f}")

    # --- æ­¥é©Ÿ 7 & 8: åˆ†æèˆ‡å„²å­˜çµæœ ---
    perform_error_analysis(X, y, oof_preds, best_threshold, ProjectConfig.RESULT_DIR)
    plot_feature_importance(feature_importances.fillna(0), ProjectConfig.RESULT_DIR)
    
    print("\n--- æ­¥é©Ÿ 8: ä½¿ç”¨æœ€çµ‚é–€æª»ç”Ÿæˆ submission.csv ---")
    predictions = (test_preds >= best_threshold).astype(int)
    submission_df = pd.DataFrame({'acct': predict_df['acct'], 'label': predictions})
    submission_path = ProjectConfig.RESULT_DIR / "submission_xgboost_advanced_feats_with_gnn.csv"
    submission_df.to_csv(submission_path, index=False)
    print(f"âœ… Submission æª”æ¡ˆå·²è¼¸å‡ºè‡³: {submission_path}")
    print("æœ€çµ‚é æ¸¬æ¨™ç±¤åˆ†ä½ˆ:\n", submission_df['label'].value_counts())

    # --- æ­¥é©Ÿ 9: å„²å­˜é æ¸¬æ©Ÿç‡ä»¥ä¾›èåˆ ---
    print("\n--- æ­¥é©Ÿ 9: å„²å­˜é æ¸¬æ©Ÿç‡ä»¥ä¾›èåˆ (Ensemble) ---")
    val_probs_series = pd.Series(oof_preds, index=X.index)
    test_probs_series = pd.Series(test_preds, index=X_test.index)

    model_name = "xgboost_gnn" 
    val_prob_path = ProjectConfig.RESULT_DIR / f'{model_name}_val_probs.pkl'
    test_prob_path = ProjectConfig.RESULT_DIR / f'{model_name}_test_probs.pkl'

    with open(val_prob_path, 'wb') as f:
        pickle.dump(val_probs_series, f)
    print(f"OOF (Validation) é æ¸¬æ©Ÿç‡å·²å„²å­˜è‡³: {val_prob_path}")

    with open(test_prob_path, 'wb') as f:
        pickle.dump(test_probs_series, f)
    print(f"Test é æ¸¬æ©Ÿç‡å·²å„²å­˜è‡³: {test_prob_path}")
    
    print("\nXGBoost æµç¨‹åŸ·è¡Œå®Œç•¢ã€‚")